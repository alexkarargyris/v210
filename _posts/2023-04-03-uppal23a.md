---
title: 'Decoding Attention from Gaze: A Benchmark Dataset and End-to-End Models'
abstract: Eye-tracking has potential to provide rich behavioral data about human cognition
  in eco- logically valid environments. However, analyzing this rich data is often
  challenging. Most automated analyses are specific to simplistic artificial visual
  stimuli with well-separated, static regions of interest, while most analyses in
  the context of complex visual stimuli, such as most natural scenes, rely on laborious
  and time-consuming manual annotation. This paper studies using computer vision tools
  for “attention decoding”, the task of assessing the locus of a participant’s overt
  visual attention over time. We provide a publicly available Multiple Object Eye-Tracking
  (MOET) dataset, consisting of gaze data from participants tracking specific objects,
  annotated with labels and bounding boxes, in crowded real-world videos, for training
  and evaluating attention decoding algorithms. We also propose two end- to-end deep
  learning models for attention decoding and compare these to state-of-the-art heuristic
  methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: uppal23a
month: 0
tex_title: 'Decoding Attention from Gaze: A Benchmark Dataset and End-to-End Models'
firstpage: 219
lastpage: 240
page: 219-240
order: 219
cycles: false
bibtex_author: Uppal, Karan and Kim, Jaeah and Singh, Shashank
author:
- given: Karan
  family: Uppal
- given: Jaeah
  family: Kim
- given: Shashank
  family: Singh
date: 2023-04-03
address:
container-title: Proceedings of The 1st Gaze Meets ML workshop
volume: '210'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 3
pdf: https://proceedings.mlr.press/v210/uppal23a/uppal23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
